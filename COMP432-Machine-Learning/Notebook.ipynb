{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Note to marker\n",
    "The dimensionality reduction and hyperparameter search took over 24h for some of the datasets even on Google Colab.<br>\n",
    "So sometimes we save progress in ***savefiles/*** folder due to disconection times of Colab.<br>\n",
    "We also included a demo dataset which you can run the whole notebook for demonstration purposes. The dimensionality reduction techniques we use are meant for datasets with over 100 dimensions, but the results are surprisingly good on this 27 attributes demo dataset.  <br>\n",
    "\n",
    "At the last moment, we found an optimization for the Combined ReliefF-Linear SVM weights technique, which drastically reduces the time to perform the reduction to be only slightly longer than Linear SVM weights. We only had time to get all updated results for the micro_mass dataset(1300 features). For the indian_pines dataset(220 features), we were able to get the reduction times, but due to the high sample size, we couldn't get the hypterparameters final results, but they shouldn't have been significantly different. For the olivetti_faces dataset(4096 features), we got the reduction times for 10 different features removal and averaged them to get an estimate. The reduction times are not affected by number of features removed, but have some small variations due to usage of randomness in the algorithms. At the end of the notebook we display these new timings for the 2 datasets, which we use in the report, along with the old values.<br>\n",
    "\n",
    "We have saved all the results from running the 3 main datasets, which can be plotted at the end of the notebook. But if you have a a day or two to spare, you can run it on one of the 3 main datasets, change the ***dataset_name*** variable below to the name of some other dataset from ***datasets*** folder. <br>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"forest_types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.spatial as sp #KDTree\n",
    "import scipy.stats  # reciprocal distribution\n",
    "import sklearn.neighbors #KNeighborsClassifier\n",
    "import sklearn.ensemble #RandomForestClassifier\n",
    "import sklearn.svm #SVC\n",
    "import sklearn.preprocessing #standarization\n",
    "import sklearn.model_selection #RandomSearchCV\n",
    "import time\n",
    "import random\n"
   ]
  },
  {
   "source": [
    "# Definition of all needed functions\n",
    "## Dimensionality reduction techniques"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReliefF(X, Y, m, k):\n",
    "    \"\"\"Weighs the ReliefF, ReliefF model definition, assumes the KDTree sorting has already been done\"\"\"\n",
    "    #initialization\n",
    "    W = np.zeros(X.shape[1])\n",
    "    trees = np.array([sp.KDTree(X[Y==i]) for i in range(len(np.unique(Y)))]) #O(cnlogn)\n",
    "    #choosing random points\n",
    "    choice = np.random.choice(np.arange(len(X)),m)\n",
    "    #maximums and minimums of all features\n",
    "    maximums, minimums = np.max(X, axis=0), np.min(X, axis=0)\n",
    "    P = np.unique(Y, return_counts = True)[1]/len(Y)\n",
    "    for i in range(m): #for each point\n",
    "        #point definition\n",
    "        x, y = X[choice[i]], Y[choice[i]]\n",
    "        current = trees[y]\n",
    "        hits = np.array(current.query(x, k=k+1)[1], dtype='i4')\n",
    "        misses = np.array([tree.query(x, k=k)[1] if tree != current else np.zeros(k) for tree in trees], dtype='i4')\n",
    "        for j in range(X.shape[1]):\n",
    "            if maximums[j] - minimums[j] <= 0.000001 and maximums[j] - minimums[j] >= -0.000001:\n",
    "                W[j] = 0\n",
    "            else:\n",
    "                W[j] += -np.sum(diff(x[j], X[hits].T[j], maximums[j], minimums[j]))/(m*k)\n",
    "                W[j] += np.sum(np.array([P[category]/(1-P[y])*sum(diff(x[j], X[misses[category]].T[j], maximums[j], minimums[j])) for \\\n",
    "                                                                        category in range(len(P)) if category != y]))/(k*m)\n",
    "    return W\n",
    "\n",
    "def diff(p1,p2,maxi,mini):\n",
    "    \"\"\"Auxiliary function to ReliefF, true to the theory of ReliefF\"\"\"\n",
    "    return np.absolute(p1-p2)/(maxi-mini)\n",
    "\n",
    "def linearSVM(X,Y): \n",
    "    \"\"\"Weighs the linear SVM weight model. Assumes SVM random search for parameters tol and C\"\"\"\n",
    "    if len(X) > X.shape[1]:\n",
    "        svm = sklearn.svm.LinearSVC(dual=False, max_iter=1000)\n",
    "    else:\n",
    "        svm = sklearn.svm.LinearSVC(max_iter=1000)\n",
    "    param_dist = {'C': np.logspace(-3, 2, 6)}\n",
    "    r_search = sklearn.model_selection.RandomizedSearchCV(svm, param_distributions={'C': np.logspace(-3, 2, 15)}, cv = 3, random_state=0)\n",
    "    r_search.fit(X,Y)\n",
    "    weights = r_search.best_estimator_.coef_\n",
    "    if (len(weights)==1):\n",
    "        return weights[0]\n",
    "    else:\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = np.argsort(np.abs(weights[i]))\n",
    "        return np.sum(weights, axis=0)\n",
    "\n",
    "def reduce_X(X, W, remove, feature_list):\n",
    "    \"\"\"\n",
    "    Reduces the dimension of X based on the sorting of the weight.\n",
    "    The weights with greatest magnitude are the most important features\n",
    "    \"\"\"\n",
    "    new = np.argsort(np.abs(W))[remove:]\n",
    "    return X[:,new], feature_list[new]\n",
    "\n",
    "def ReliefFSelect(X, Y, m, k, remove, feature_list):\n",
    "    \"\"\"ReliefF Feature Selection Model\"\"\"\n",
    "    return reduce_X(X, ReliefF(X, Y, m, k), remove, feature_list)\n",
    "\n",
    "def linearSVMWeightSelect(X,Y, remove, feature_list):\n",
    "    \"\"\"Linear SVM Weight Feature Selection Model\"\"\"\n",
    "    return reduce_X(X, linearSVM(X,Y), remove, feature_list)\n",
    "\n",
    "def combinedReliefFLinearSVM(X, Y, m, k, part, total, feature_list):\n",
    "    \"\"\"\n",
    "    Main model, combination of 2 models\n",
    "    Multilayered feature selection\n",
    "\n",
    "    FUNCTION OF INTEREST\n",
    "    RRFE implementation of the algorithm.\n",
    "    \"\"\"\n",
    "    # first layer. Linear SVM Weight Feature Selection\n",
    "    X, feature_list = linearSVMWeightSelect(X,Y, part, feature_list)\n",
    "    # second layer, ReliefF\n",
    "    return ReliefFSelect(X, Y, m, k, total-part, feature_list)\n",
    "\n",
    "# OLD VERSION OF combinedReliefFLinearSVM before optimization\n",
    "\"\"\"\n",
    "def combinedReliefFLinearSVM(X, Y, m, k, part, total, feature_list):\n",
    "    # first layer\n",
    "    #RFE Linear SVM Weight Feature Selection\n",
    "    i=0\n",
    "    while RFE_step*i < part:\n",
    "        X, feature_list = linearSVMWeightSelect(X,Y, RFE_step, feature_list)\n",
    "        i += 1\n",
    "    # second layer\n",
    "    return ReliefFSelect(X, Y, m, k, total-i*RFE_step, feature_list)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def dim_reduction(X, y, RFsamples, partial_remove, total_remove):\n",
    "    \"\"\"\n",
    "    Applies ReliefF, Linear-SVM-Weight and Combined ReliefF-Linear-SVM-Weight on X,y\n",
    "    Outputs the column indexes that are kept after dimension reduction, as well as time it took to perform the reduction.\n",
    "    Returns 3x2 list, representing the results (remaining features and time to reduce) for each of the 3 dimensional reduction techniques.\n",
    "    Output: [[feature_list_RF,      reduction_time_RF],\n",
    "             [feature_list_SVM,     reduction_time_SVM],\n",
    "             [feature_list_RFSVM,   reduction_time_RFSVM]]\n",
    "\n",
    "    RFsamples:      number of samples used to estimate ReliefF, can be safely 100, for smaller datasets, at least 1% of samples.\n",
    "    partial_remove: number of features removed by SVM, for combinedSVMRF only\n",
    "    total_remove:   total number of features removed\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nReducing dimensions with: RFsamples={}, partial_remove={}, total_remove={}\".format(RFsamples, partial_remove, total_remove))\n",
    "    # Apply ReliefF\n",
    "    timer = time.time()\n",
    "    f_l_RF = np.arange(X.shape[1])\n",
    "    X_ReliefF, f_l_RF = ReliefFSelect(X, y, RFsamples, 6, total_remove, f_l_RF)\n",
    "    time_RF = time.time() - timer\n",
    "    print(\"ReliefF took {:.1f} minutes\".format(time_RF/60))\n",
    "\n",
    "    # Apply Linear SVM weight\n",
    "    timer = time.time()\n",
    "    f_l_SVM = np.arange(X_train.shape[1])\n",
    "    X_SVM, f_l_SVM = linearSVMWeightSelect(X,y, total_remove, f_l_SVM)\n",
    "    time_SVM = time.time() - timer\n",
    "    print(\"LiSVM took {:.1f} minutes\".format(time_SVM/60))\n",
    "\n",
    "    # Apply combined ReliefF-Linear SVM weight\n",
    "    timer = time.time()\n",
    "    f_l_RFSVM = np.arange(X_train.shape[1])\n",
    "    X_RFSVM, f_l_RFSVM = combinedReliefFLinearSVM(X, y, RFsamples, 6, partial_remove, total_remove, f_l_RFSVM)\n",
    "    time_RFSVM = time.time() - timer\n",
    "    print(\"Combined ReliefF-LiSVM took {:.1f} minutes\".format(time_RFSVM/60))\n",
    "\n",
    "    print(\"\\nTotal time taken for removing {} dims: {:.1f} minutes\".format(total_remove, (time_RF+time_SVM+time_RFSVM)/60))\n",
    "    result = [[f_l_RF, int(time_RF)],\n",
    "              [f_l_SVM, int(time_SVM)],\n",
    "              [f_l_RFSVM, int(time_RFSVM)]]\n",
    "    return result\n"
   ]
  },
  {
   "source": [
    "## Data loading and training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(filename):\n",
    "    \"\"\"Loads dataset from filename, returns train/test data and targets.\"\"\"\n",
    "    data = np.loadtxt(filename, skiprows=1, delimiter=',')\n",
    "    data_train, data_test = sklearn.model_selection.train_test_split(data, test_size=0.3, random_state=0)\n",
    "    X_train, y_train = data_train[:,:-1], data_train[:,-1].astype('i4')\n",
    "    X_test, y_test = data_test[:,:-1], data_test[:,-1].astype('i4')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def dim_reduction_results(X_train, X_test, y_train, y_test, model, features_indexes):\n",
    "  '''\n",
    "  For each of the 3 dimension reduction techniques, train model on reduced features of X_train according to features_indexes.\n",
    "  features_indexes: 3xN array containing indexes of N features we are to keep from X\n",
    "  Returns score on reduced test data as well as time it took to train model in a 3x2 array\n",
    "  '''\n",
    "  #ReliefF\n",
    "  X_train_reduced = X_train[:,features_indexes[0]]\n",
    "  X_test_reduced = X_test[:,features_indexes[0]]\n",
    "  timer = time.time()\n",
    "  model.fit(X_train_reduced, y_train)\n",
    "  time_RF = time.time() - timer\n",
    "  score_RF = model.score(X_test_reduced, y_test)\n",
    "\n",
    "  #Linear-SVM-Weights\n",
    "  X_train_reduced = X_train[:,features_indexes[1]]\n",
    "  X_test_reduced = X_test[:,features_indexes[1]]\n",
    "  timer = time.time()\n",
    "  model.fit(X_train_reduced, y_train)\n",
    "  time_SVM = time.time() - timer\n",
    "  score_SVM = model.score(X_test_reduced, y_test)\n",
    "\n",
    "  #Combined ReliefF-Linear-SVM-Weights\n",
    "  X_train_reduced = X_train[:,features_indexes[2]]\n",
    "  X_test_reduced = X_test[:,features_indexes[2]]\n",
    "  timer = time.time()\n",
    "  model.fit(X_train_reduced, y_train)\n",
    "  time_RFSVM = time.time() - timer\n",
    "  score_RFSVM = model.score(X_test_reduced, y_test)\n",
    "\n",
    "  result = np.array(\n",
    "      [[score_RF, np.int(time_RF)],\n",
    "       [score_SVM, np.int(time_SVM)],\n",
    "       [score_RFSVM, np.int(time_RFSVM)]], dtype=object)\n",
    "  return result"
   ]
  },
  {
   "source": [
    "## Plotting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_scores(num_features, reduced_scores, unreduced_scores, title=None):\n",
    "  \"\"\"\n",
    "  plots accuracy for reduced features data, as well as a horizontal line representing unreduced accuracy\n",
    "  \"\"\"\n",
    "  max_scores = np.max(reduced_scores, axis=0)\n",
    "  best_num_features = num_features[np.argmax(reduced_scores, axis=0)]\n",
    "\n",
    "  plt.figure(figsize=(max(len(num_features)/10,7.5),5))\n",
    "  #plt.figure()\n",
    "  plt.scatter(best_num_features[0], max_scores[0], c='k', marker='x', s=75, linewidth=3)\n",
    "  plt.scatter(best_num_features[1], max_scores[1], c='k', marker='x', s=75, linewidth=3)\n",
    "  plt.scatter(best_num_features[2], max_scores[2], c='k', marker='x', s=75, linewidth=3)\n",
    "  plt.plot(num_features, reduced_scores[:,0], c='b', label='ReliefF')\n",
    "  plt.plot(num_features, reduced_scores[:,1], c='r', label='LiSVM')\n",
    "  plt.plot(num_features, reduced_scores[:,2], c='g', label='ReliefF+LiSVM')\n",
    "  plt.axhline(unreduced_scores, c='brown', label='Unreduced')\n",
    "\n",
    "  plt.text(0.4 * (max(num_features)+1), 0.20,'Best RF score:{:.3f} at {} features'.format(max_scores[0], best_num_features[0]), c='b', ha='left')\n",
    "  plt.text(0.4 * (max(num_features)+1), 0.15,'Best LiSVM score:{:.3f} at {} features'.format(max_scores[1], best_num_features[1]), c='r', ha='left')\n",
    "  plt.text(0.4 * (max(num_features)+1), 0.10,'Best RF-LiSVM score:{:.3f} at {} features'.format(max_scores[2], best_num_features[2]), c='g', ha='left')\n",
    "  plt.text(0.4 * (max(num_features)+1), 0.05,'Unreduced score:{:.3f}'.format(unreduced_scores), c='k', ha='left')\n",
    "\n",
    "  plt.xticks(np.arange(0, max(num_features)+1, 5))\n",
    "  plt.yticks(np.linspace(0,1,11))\n",
    "  #plt.ylim(0,1)\n",
    "  plt.xlabel(\"Number of features\")\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.title(title)\n",
    "  plt.legend(loc='best')\n",
    "\n",
    "def plot_timings(num_features, timings, labels, unreduced=None, title=None):\n",
    "  \"\"\"\n",
    "  general plotting for timings. Used for time taken to reduce dimensions and time to train a model\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(max(len(num_features)/10,7.5),5))\n",
    "  #plt.figure()\n",
    "  plt.plot(num_features, timings[:,0], c='b', label='ReliefF')\n",
    "  plt.plot(num_features, timings[:,1], c='r', label='LiSVM')\n",
    "  plt.plot(num_features, timings[:,2], c='g', label='Multi-Layers(ReliefF+LiSVM)')\n",
    "  if unreduced != None:\n",
    "    plt.axhline(unreduced, c='brown', label='Unreduced')\n",
    "  \n",
    "  plt.xticks(np.arange(0, max(num_features)+1, 5))\n",
    "  plt.xlabel(labels[0])\n",
    "  plt.ylabel(labels[1])\n",
    "  plt.title(title)\n",
    "  plt.legend(loc='best')"
   ]
  },
  {
   "source": [
    "# Perform dimensional reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "X_train, X_test, y_train, y_test = prep_data('datasets/{}.csv'.format(dataset_name))\n",
    "#standardize\n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(\"Shape of train data: {}\".format(X_train.shape))\n",
    "print(\"Shape of test data: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check by running dummy classifiers\n",
    "dummies = []\n",
    "dummies.append(sklearn.dummy.DummyClassifier('stratified').fit(X_train, y_train))\n",
    "dummies.append(sklearn.dummy.DummyClassifier('most_frequent').fit(X_train, y_train))\n",
    "dummies.append(sklearn.dummy.DummyClassifier('prior').fit(X_train, y_train))\n",
    "dummies.append(sklearn.dummy.DummyClassifier('uniform').fit(X_train, y_train))\n",
    "\n",
    "for dummy in dummies:\n",
    "    print('{} dummy train score: {:.2f}'.format(dummy.strategy, dummy.score(X_train, y_train)))\n",
    "    print('{} dummy test score: {:.2f}\\n'.format(dummy.strategy, dummy.score(X_test, y_test)))"
   ]
  },
  {
   "source": [
    "## Setup dimension reduction parameters\n",
    "We show, but comment out the parameters we used for reducing the datasets from the paper."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) #for reproducing results\n",
    "#TO MODIFY DEPENDING ON THE DATASET\n",
    "\n",
    "#number of samples used to estimate ReliefF, can be safely 100. For smaller datasets, at least 1% of samples.\n",
    "RFsamples = 30\n",
    "#number of features removed by SVM, for combinedRFSVM only\n",
    "partial_remove = 5\n",
    "#maximum number of features removed\n",
    "max_total_remove = 5 #or X_train.shape[1]-1\n",
    "\n",
    "\n",
    "#indian_pines\n",
    "#RFsamples = 80\n",
    "#partial_remove = 120\n",
    "\n",
    "#micro_mass\n",
    "#RFsamples = 40\n",
    "#partial_remove = 700\n",
    "\n",
    "#olivetti_faces (optimized)\n",
    "#RFsamples = 40\n",
    "#partial_remove = 3500\n",
    "\n",
    "#olivetti_faces (non-optimized as displayed in results later):\n",
    "#note the extra parameter RFE_step which was only present in old version of combinedReliefFLinearSVM (commented out above)\n",
    "#RFE_step = 500\n",
    "#RFsamples = 40\n",
    "##partial_remove = 3500\n",
    "\n",
    "assert max_total_remove < X_train.shape[1], \"max_total_remove can't be >= total number of features!\""
   ]
  },
  {
   "source": [
    "### Reduce data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict instead of list since for olivetti_faces dataset, I had to compute for every other number of feature and needed to know for which I computed\n",
    "results = {}\n",
    "#get results from reducing to 1 feature, up to reducing max_total_remove features\n",
    "for total_remove in range(X_train.shape[1]-1, X_train.shape[1]-max_total_remove, -1):\n",
    "  results[X_train.shape[1]-total_remove] = dim_reduction(X_train, y_train, RFsamples, partial_remove, total_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results. Yes saving dictionary as .npy\n",
    "np.save(\"savefiles/reduction_{}_{}_{}.npy\".format(dataset_name, RFsamples, partial_remove), results)"
   ]
  },
  {
   "source": [
    "# Individual testing\n",
    "Use cell below to test removal of specific number of dimensions. Mainly used for testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_remove = 10\n",
    "assert total_remove < X_train.shape[1] and total_remove > 0, \"total remove must be larger than 0 and lower than total number of features!\"\n",
    "print('This is the preprocessed data:\\n{}\\n\\n'.format(X_train))\n",
    "\n",
    "# Test ReliefF\n",
    "f_l_RF = np.arange(X_train.shape[1])\n",
    "timer = time.time()\n",
    "X_ReliefF, f_l_RF = ReliefFSelect(X_train, y_train, RFsamples, 6, total_remove, f_l_RF)\n",
    "print(\"Relieff took {:.0f} seconds\".format(time.time() - timer))\n",
    "print('This is the data chosen by the ReliefF:\\n{}'.format(X_ReliefF))\n",
    "print('These are the features selected:\\n{}\\n\\n'.format(f_l_RF))\n",
    "\n",
    "# Test linearSVM\n",
    "f_l_SVM = np.arange(X_train.shape[1])\n",
    "timer = time.time()\n",
    "X_SVM, f_l_SVM = linearSVMWeightSelect(X_train,y_train, total_remove, f_l_SVM)\n",
    "print(\"SVM took {:.0f} seconds\".format(time.time() - timer))\n",
    "print('This is the data chosen by the Linear SVM weights:\\n{}'.format(X_SVM))\n",
    "print('These are the features selected:\\n{}\\n\\n'.format(f_l_SVM))\n",
    "\n",
    "# Test combined ReliefF-LinearSVM\n",
    "f_l_RFSVM = np.arange(X_train.shape[1])\n",
    "timer = time.time()\n",
    "X_RFSVM, f_l_RFSVM = combinedReliefFLinearSVM(X_train, y_train, RFsamples, 6, partial_remove, total_remove, f_l_RFSVM)\n",
    "print(\"RelieffSVM took {:.0f} seconds\".format(time.time() - timer))\n",
    "print('This is the data chosen by the combined ReliefF-Linear SVM weights:\\n{}'.format(X_RFSVM))\n",
    "print('These are the features selected:\\n{}'.format(f_l_RFSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Hyperparameter search\n",
    "### Load results from dimension reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "config contains the feature indexes we keep and how long it took to reduce to them. It's setup in the following way:\n",
    "key: number of features we use after dimension reduction\n",
    "value: [[[indexes_RF], reduced_time_ReliefF],\n",
    "        [[indexes_LiSVM], reduced_time_LiSVM],\n",
    "        [[indexes_RFSVM], reduced_time_RFSVM]]\n",
    "'''\n",
    "config = np.load(\"savefiles/reduction_{}_{}_{}.npy\".format(dataset_name, RFsamples, partial_remove), allow_pickle=True)[()]\n",
    "#load usefull data from config\n",
    "num_features = (np.array(list(config.keys())))\n",
    "reduced_idx = np.array(list(config.values()))[:,:,0]\n",
    "reduced_time = np.array(list(config.values()))[:,:,1]\n",
    "print(random.choice(list(config.items())))"
   ]
  },
  {
   "source": [
    "## RandomForestClassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameter search\n",
    "model_forest = sklearn.ensemble.RandomForestClassifier(random_state=0)\n",
    "#param_distribution = {'max_depth':scipy.stats.randint(1,60), 'n_estimators':scipy.stats.randint(1,200)}\n",
    "#forest_rsCV = sklearn.model_selection.RandomizedSearchCV(model_fore\n",
    "\n",
    "#the above was used on the project datasets, but it's too much for the demo dataset, so we run the following instead\n",
    "param_distribution = {'max_depth':scipy.stats.randint(1,10), 'n_estimators':scipy.stats.randint(1,25)}\n",
    "forest_rsCV = sklearn.model_selection.RandomizedSearchCV(model_forest, param_distributions=param_distribution, n_iter=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_results_forest = []\n",
    "for features, idx in zip(num_features, reduced_idx):\n",
    "  print('Training and scoring on {} features...'.format(features))\n",
    "  results = dim_reduction_results(X_train, X_test, y_train, y_test, forest_rsCV, idx)\n",
    "  reduction_results_forest.append(results)\n",
    "reduction_results_forest = np.array(reduction_results_forest, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reduction_results_forest has the following structure, and this will also apply to SVC and KNN\n",
    "[[RF_score, RF_train_time],\n",
    " [SVM_score, SVM_train_time],\n",
    " [RFSVM_score, RFSVM_train_time]]\n",
    "'''\n",
    "#separate scoring results from train_time results\n",
    "reduced_scores_forest = reduction_results_forest[:,:,0]\n",
    "reduced_train_time_forest = reduction_results_forest[:,:,1]\n",
    "print(random.choice(reduction_results_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter search using original unreduced data\n",
    "timer = time.time()\n",
    "forest_rsCV.fit(X_train,y_train)\n",
    "unreduced_score_forest = forest_rsCV.score(X_test,y_test)\n",
    "unreduced_train_time_forest = np.int(time.time() - timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_scores(num_features, reduced_scores_forest, unreduced_score_forest)\n",
    "plt.savefig('savefigs/{}_RandomForestClassifier'.format(dataset_name))"
   ]
  },
  {
   "source": [
    "## SVC\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameter search\n",
    "model = sklearn.svm.SVC(random_state=0)\n",
    "#reciprocal_distribution = scipy.stats.reciprocal(0.01, 250)\n",
    "reciprocal_distribution = scipy.stats.reciprocal(0.1, 10)\n",
    "param_distribution = {'C':reciprocal_distribution}\n",
    "#svc_rsCV = sklearn.model_selection.RandomizedSearchCV(model, param_distributions=param_distribution, random_state=0, n_iter=50)\n",
    "svc_rsCV = sklearn.model_selection.RandomizedSearchCV(model, param_distributions=param_distribution, random_state=0, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_results_svc = []\n",
    "for features, idx in zip(num_features, reduced_idx):\n",
    "  print('Training and scoring on {} features...'.format(features))\n",
    "  results = dim_reduction_results(X_train, X_test, y_train, y_test, svc_rsCV, idx)\n",
    "  reduction_results_svc.append(results)\n",
    "reduction_results_svc = np.array(reduction_results_svc, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_scores_svc = reduction_results_svc[:,:,0]\n",
    "reduced_train_time_svc = reduction_results_svc[:,:,1]\n",
    "print(random.choice(reduction_results_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = time.time()\n",
    "svc_rsCV = svc_rsCV.fit(X_train,y_train)\n",
    "unreduced_score_svc = svc_rsCV.score(X_test,y_test)\n",
    "unreduced_train_time_svc = np.int(time.time() - timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_scores(num_features, reduced_scores_svc, unreduced_score_svc)\n",
    "plt.savefig('savefigs/{}_SVC'.format(dataset_name))"
   ]
  },
  {
   "source": [
    "# KNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.neighbors.KNeighborsClassifier()\n",
    "#param_distribution = {\"n_neighbors\":scipy.stats.randint(1,200)}\n",
    "param_distribution = {\"n_neighbors\":scipy.stats.randint(1,25)}\n",
    "knn_gsCV = sklearn.model_selection.RandomizedSearchCV(model, param_distributions=param_distribution, random_state=0, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_results_knn = []\n",
    "for features, idx in zip(num_features, reduced_idx):\n",
    "  print('Training and scoring on {} features...'.format(features))\n",
    "  results = dim_reduction_results(X_train, X_test, y_train, y_test, knn_gsCV, idx)\n",
    "  reduction_results_knn.append(results)\n",
    "reduction_results_knn = np.array(reduction_results_knn, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_scores_knn = reduction_results_knn[:,:,0]\n",
    "reduced_train_time_knn = reduction_results_knn[:,:,1]\n",
    "print(random.choice(reduction_results_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = time.time()\n",
    "knn_gsCV = knn_gsCV.fit(X_train,y_train)\n",
    "unreduced_score_knn = knn_gsCV.score(X_test,y_test)\n",
    "unreduced_train_time_knn = np.int(time.time() - timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_scores(num_features, reduced_scores_knn, unreduced_score_knn)\n",
    "plt.savefig('savefigs/{}_KNN'.format(dataset_name))"
   ]
  },
  {
   "source": [
    "## Save results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({ 'num_features':num_features,\\\n",
    "                    'RF_indexes':reduced_idx[:,0],\\\n",
    "                    'SVM_indexes':reduced_idx[:,1],\\\n",
    "                    'RFSM_indexes':reduced_idx[:,2],\\\n",
    "                    'RF_time':reduced_time[:,0],\\\n",
    "                    'SVM_time':reduced_time[:,1],\\\n",
    "                    'RFSM_time':reduced_time[:,2],\\\n",
    "                    \\\n",
    "                    'unreduced_score_forest':unreduced_score_forest,\\\n",
    "                    'RF_scores_forest':reduced_scores_forest[:,0],\\\n",
    "                    'SVM_scores_forest':reduced_scores_forest[:,1],\\\n",
    "                    'RFSVM_scores_forest':reduced_scores_forest[:,2],\\\n",
    "                    \\\n",
    "                    'unreduced_train_time_forest':unreduced_train_time_forest,\\\n",
    "                    'RF_train_time_forest':reduced_train_time_forest[:,0],\\\n",
    "                    'SVM_train_time_forest':reduced_train_time_forest[:,1],\\\n",
    "                    'RFSVM_train_time_forest':reduced_train_time_forest[:,2],\\\n",
    "                    \\\n",
    "                    'unreduced_score_svc':unreduced_score_svc,\\\n",
    "                    'RF_scores_svc':reduced_scores_svc[:,0],\\\n",
    "                    'SVM_scores_svc':reduced_scores_svc[:,1],\\\n",
    "                    'RFSVM_scores_svc':reduced_scores_svc[:,2],\\\n",
    "                    \\\n",
    "                    'unreduced_train_time_svc':unreduced_train_time_svc,\\\n",
    "                    'RF_train_time_svc':reduced_train_time_svc[:,0],\\\n",
    "                    'SVM_train_time_svc':reduced_train_time_svc[:,1],\\\n",
    "                    'RFSVM_train_time_svc':reduced_train_time_svc[:,2],\\\n",
    "                    \\\n",
    "                    'unreduced_score_knn':unreduced_score_knn,\\\n",
    "                    'RF_scores_knn':reduced_scores_knn[:,0],\\\n",
    "                    'SVM_scores_knn':reduced_scores_knn[:,1],\\\n",
    "                    'RFSVM_scores_knn':reduced_scores_knn[:,2],\\\n",
    "                    \\\n",
    "                    'unreduced_train_time_knn':unreduced_train_time_knn,\\\n",
    "                    'RF_train_time_knn':reduced_train_time_knn[:,0],\\\n",
    "                    'SVM_train_time_knn':reduced_train_time_knn[:,1],\\\n",
    "                    'RFSVM_train_time_knn':reduced_train_time_knn[:,2]})\n",
    "df.to_csv('savefiles/final_results_{}.csv'.format(dataset_name), index=None)"
   ]
  },
  {
   "source": [
    "# Plot results\n",
    "The results for all project datasets are already saved in savefiles folder<br>\n",
    "Uncomment the line for which dataset you want to plot.<br><br>\n",
    "As stated at the begining of notebook, the Combined ReliefF-Linear SVM weight had a last minute optimization. But we only had time to apply it to the micro_mass and indina_pines datasets. Thus, the olivetti_faces datasets have much higher time for reduction under Combined ReliefF-Linear SVM weight. We make an estimate for correct reduction time at the end of the notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_name = 'forest_types'\n",
    "#dataset_name = 'indian_pines'\n",
    "#dataset_name = 'micro_mass'\n",
    "dataset_name = 'olivetti_faces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the final_results from csv file to numpy, then to appropriate variables.\n",
    "I saved the feature indexes we keep as strings in the csv file, so I'll just convert them back to numpy arrays\n",
    "\"\"\"\n",
    "def strArray_to_npArray(str):\n",
    "    elements = str[1:-1].split()\n",
    "    return np.array(elements, dtype=np.int)\n",
    "\n",
    "results = pd.read_csv('savefiles/final_results_{}.csv'.format(dataset_name)).to_numpy()\n",
    "results[:,1] = np.array(list(map(strArray_to_npArray, results[:,1])))\n",
    "results[:,2] = np.array(list(map(strArray_to_npArray, results[:,2])))\n",
    "results[:,3] = np.array(list(map(strArray_to_npArray, results[:,3])))\n",
    "\n",
    "reduced_scores_forest = np.column_stack([results[:,8], results[:,9], results[:,10]])\n",
    "reduced_scores_forest = reduced_scores_forest.reshape(-1,3)\n",
    "\n",
    "reduced_scores_svc = np.column_stack([results[:,16], results[:,17], results[:,18]])\n",
    "reduced_scores_svc = reduced_scores_svc.reshape(-1,3)\n",
    "\n",
    "reduced_scores_knn = np.column_stack([results[:,24], results[:,25], results[:,26]])\n",
    "reduced_scores_knn = reduced_scores_knn.reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timings(results[:,0], results[:,4:7], ['Number of features', 'Time(seconds)'], title=\"Time to reduce relative to final number of features (4096 features set)\")\n",
    "#plt.savefig('savefigs/reduction_time_{}'.format(dataset_name))\n",
    "plot_features_scores(results[:,0], reduced_scores_forest, results[0,7], \"{} using RandomForestClassifier\".format(dataset_name))\n",
    "plot_features_scores(results[:,0], reduced_scores_svc, results[0,15], \"{} using SVC\".format(dataset_name))\n",
    "plot_features_scores(results[:,0], reduced_scores_knn, results[0,23], \"{} using KNN\".format(dataset_name))"
   ]
  },
  {
   "source": [
    "# Get estimate for time to reduce data with improved Combined ReliefF-LiSVM\n",
    "Here we estimate the reduction times for the olivetti_faces dataset (4086 attribs) with the improved Combined ReliefF_SVM <br>\n",
    "Because the time taken to reduce is close to constant, we averaged the reduction times for 10 different feature reductions. We can observe that the RF and LiSVM haven't changed significantly, but we got very large improvements on the RF-LiSVM method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_old = pd.read_csv('savefiles/final_results_olivetti_faces.csv').to_numpy()\n",
    "reduced_time = results[:,4:7]\n",
    "avg_reduced_time = np.average(reduced_time, axis=0)\n",
    "print(\"Average time to reduce olivetti_faces under non-optimal Combined ReliefF-LiSVM:\")\n",
    "print(\"RF:{}\\nLiSVM:{}\\nRF-LiSVM:{}\\n\\n\".format(avg_reduced_time[0], avg_reduced_time[1], avg_reduced_time[2]))\n",
    "print(avg_reduced_time)\n",
    "\n",
    "config_new = np.load('savefiles/some_reduction_olivetti_faces_40_3500.npy', allow_pickle=True)[()]\n",
    "reduced_time = np.array(list(config_new.values()))[:,:,1]\n",
    "avg_reduced_time = np.average(reduced_time, axis=0)\n",
    "print(\"Average time to reduce olivetti_faces under optimized Combined ReliefF-LiSVM:\")\n",
    "print(\"RF:{}\\nLiSVM:{}\\nRF-LiSVM:{}\\n\\n\".format(avg_reduced_time[0], avg_reduced_time[1], avg_reduced_time[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}